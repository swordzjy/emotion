"""
è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« v2.1 - å®æ—¶æµå¼ç‰ˆæœ¬
ç‰¹æ€§ï¼š
- å®æ—¶ VAD æ£€æµ‹ï¼Œè¯­éŸ³ç»“æŸè‡ªåŠ¨åœæ­¢
- æ— éœ€å›ºå®šå½•éŸ³æ—¶é•¿
- æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
"""

import pyaudio
import numpy as np
import torch
import time
import warnings
warnings.filterwarnings("ignore")

# ============== é…ç½®å‚æ•° ==============
SAMPLE_RATE = 16000
CHUNK = 512  # Silero VAD æ¨è
MAX_RECORD_SECONDS = 30  # æœ€å¤§å½•éŸ³æ—¶é•¿
SILENCE_THRESHOLD_SEC = 1.5  # é™éŸ³å¤šä¹…ååœæ­¢å½•éŸ³
MIN_SPEECH_SEC = 0.5  # æœ€çŸ­æœ‰æ•ˆè¯­éŸ³

WHISPER_MODEL = "base"

# ============== åŠ è½½æ¨¡å‹ ==============
print("=" * 50)
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

# Silero VAD
print("[1/3] Silero VAD...")
vad_model, utils = torch.hub.load(
    'snakers4/silero-vad', 'silero_vad', force_reload=False, onnx=False
)
(get_speech_timestamps, _, _, VADIterator, collect_chunks) = utils
vad_iterator = VADIterator(vad_model, sampling_rate=SAMPLE_RATE)
print("âœ“ Silero VAD")

# Whisper
print("[2/3] Whisper...")
import whisper
whisper_model = whisper.load_model(WHISPER_MODEL)
print(f"âœ“ Whisper ({WHISPER_MODEL})")

# æƒ…æ„Ÿè¯†åˆ«
print("[3/3] æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹...")
import sys
sys.modules['speechbrain.integrations.nlp.flair_embeddings'] = None
from speechbrain.inference.interfaces import foreign_class

emotion_classifier = foreign_class(
    source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
    pymodule_file="custom_interface.py",
    classname="CustomEncoderWav2vec2Classifier",
    savedir="pretrained_models/emotion-recognition-wav2vec2-IEMOCAP",
    run_opts={"device": "cpu"}
)
print("âœ“ æƒ…æ„Ÿè¯†åˆ«")
print("=" * 50 + "\n")


class RealtimeVADRecorder:
    """å®æ—¶ VAD å½•éŸ³å™¨"""
    
    def __init__(self):
        self.sample_rate = SAMPLE_RATE
        self.chunk = CHUNK
        
    def record_until_silence(self):
        """å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ"""
        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk
        )
        
        print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼ˆè¯´è¯ååœé¡¿å°†è‡ªåŠ¨ç»“æŸï¼‰...")
        print("   æœ€é•¿å½•éŸ³: {}ç§’ | é™éŸ³é˜ˆå€¼: {}ç§’".format(
            MAX_RECORD_SECONDS, SILENCE_THRESHOLD_SEC
        ))
        
        audio_chunks = []
        speech_detected = False
        silence_start = None
        start_time = time.time()
        
        vad_iterator.reset_states()
        
        while True:
            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > MAX_RECORD_SECONDS:
                print("\nâ±ï¸ è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿")
                break
            
            # è¯»å–éŸ³é¢‘
            data = stream.read(self.chunk, exception_on_overflow=False)
            audio_int16 = np.frombuffer(data, dtype=np.int16)
            audio_float = audio_int16.astype(np.float32) / 32768.0
            audio_tensor = torch.from_numpy(audio_float)
            
            audio_chunks.append(audio_float)
            
            # VAD æ£€æµ‹
            speech_dict = vad_iterator(audio_tensor)
            
            if speech_dict:
                if 'start' in speech_dict:
                    if not speech_detected:
                        print("   ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹...")
                    speech_detected = True
                    silence_start = None
                    
                if 'end' in speech_dict:
                    print("   ğŸ¤« æ£€æµ‹åˆ°è¯­éŸ³æ®µç»“æŸ")
                    silence_start = time.time()
            
            # æ£€æŸ¥é™éŸ³æ—¶é•¿
            if speech_detected and silence_start:
                silence_duration = time.time() - silence_start
                if silence_duration >= SILENCE_THRESHOLD_SEC:
                    print(f"\nâœ“ é™éŸ³ {SILENCE_THRESHOLD_SEC}sï¼Œåœæ­¢å½•éŸ³")
                    break
            
            # æ˜¾ç¤ºè¿›åº¦
            elapsed = time.time() - start_time
            status = "ğŸ—£ï¸" if speech_detected and not silence_start else "â³"
            print(f"\r   {status} å½•éŸ³ä¸­: {elapsed:.1f}s", end="", flush=True)
        
        print()
        stream.stop_stream()
        stream.close()
        p.terminate()
        
        # åˆå¹¶éŸ³é¢‘
        if not audio_chunks:
            return None
        
        audio = np.hstack(audio_chunks)
        total_duration = len(audio) / self.sample_rate
        print(f"âœ“ å½•éŸ³å®Œæˆ: {total_duration:.2f}ç§’")
        
        if not speech_detected:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³")
            return None
        
        return torch.from_numpy(audio)


def process_audio(audio_tensor):
    """å¤„ç†éŸ³é¢‘å¹¶åˆ†æ"""
    
    # VAD æå–æœ‰æ•ˆè¯­éŸ³
    print("\n" + "-" * 40)
    print("ã€VAD å¤„ç†ã€‘")
    timestamps = get_speech_timestamps(
        audio_tensor, vad_model, 
        sampling_rate=SAMPLE_RATE,
        threshold=0.5,
        min_speech_duration_ms=250
    )
    
    if not timestamps:
        print("âŒ VAD æœªæå–åˆ°æœ‰æ•ˆè¯­éŸ³")
        return
    
    speech_audio = collect_chunks(timestamps, audio_tensor)
    speech_duration = len(speech_audio) / SAMPLE_RATE
    print(f"âœ“ æå–æœ‰æ•ˆè¯­éŸ³: {speech_duration:.2f}ç§’ ({len(timestamps)} æ®µ)")
    
    # ä¿å­˜éŸ³é¢‘
    try:
        import soundfile as sf
        sf.write("recording.wav", speech_audio.numpy(), SAMPLE_RATE)
        print("âœ“ å·²ä¿å­˜: recording.wav")
    except:
        pass
    
    # Whisper è½¬å½•
    print("\n" + "-" * 40)
    print("ã€Whisper è½¬å½•ã€‘")
    result = whisper_model.transcribe(speech_audio.numpy(), fp16=False)
    transcript = result['text'].strip()
    language = result.get('language', 'unknown')
    print(f"è¯­è¨€: {language}")
    print(f"è½¬å½•: {transcript}")
    
    # æƒ…æ„Ÿåˆ†æ
    print("\n" + "-" * 40)
    print("ã€è¯­éŸ³æƒ…æ„Ÿã€‘")
    
    emotion_labels = ["ä¸­æ€§", "ç”Ÿæ°”", "å¼€å¿ƒ", "æ‚²ä¼¤"]
    
    prob, score, index, emotion = emotion_classifier.classify_batch(
        speech_audio.unsqueeze(0)
    )
    
    pred_idx = index[0].item()
    pred_score = score[0].item()
    
    print(f"é¢„æµ‹: {emotion_labels[pred_idx]} ({emotion[0]}) | ç½®ä¿¡åº¦: {pred_score:.2%}")
    print("\næ¦‚ç‡åˆ†å¸ƒ:")
    for i, label in enumerate(emotion_labels):
        bar = "â–ˆ" * int(prob[0][i] * 20)
        print(f"  {label}: {bar} {prob[0][i]:.1%}")
    
    # æ–‡æœ¬æƒ…æ„Ÿ
    if transcript:
        print("\n" + "-" * 40)
        print("ã€æ–‡æœ¬æƒ…æ„Ÿã€‘")
        try:
            from snownlp import SnowNLP
            s = SnowNLP(transcript)
            sent = "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
            print(f"SnowNLP: {s.sentiments:.2%} â†’ {sent}")
        except:
            pass
    
    # æ€»ç»“
    print("\n" + "=" * 40)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("=" * 40)
    print(f"  å†…å®¹: {transcript}")
    print(f"  è¯­éŸ³æƒ…ç»ª: {emotion_labels[pred_idx]} ({pred_score:.0%})")
    
    return {
        'transcript': transcript,
        'emotion': emotion_labels[pred_idx],
        'confidence': pred_score
    }


def main():
    recorder = RealtimeVADRecorder()
    
    while True:
        print("\n" + "=" * 50)
        input("æŒ‰ Enter å¼€å§‹å½•éŸ³ï¼ˆCtrl+C é€€å‡ºï¼‰...")
        
        audio = recorder.record_until_silence()
        
        if audio is not None and len(audio) > SAMPLE_RATE * MIN_SPEECH_SEC:
            result = process_audio(audio)
        else:
            print("âš ï¸ éŸ³é¢‘å¤ªçŸ­ï¼Œè¯·é‡è¯•")
        
        print("\n" + "-" * 50)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºé€€å‡º")
