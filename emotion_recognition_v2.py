"""
è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç¨‹åº v2.0
ä½¿ç”¨ Silero VADï¼ˆè½»é‡ã€å‡†ç¡®ï¼‰+ Whisperï¼ˆè½¬å½•æ•ˆæœæ›´å¥½ï¼‰
"""

import pyaudio
import numpy as np
import torch
import warnings
warnings.filterwarnings("ignore")

# ============== é…ç½®å‚æ•° ==============
SAMPLE_RATE = 16000
RECORD_SECONDS = 8
CHUNK = 512  # Silero VAD æ¨èçš„å—å¤§å°
WHISPER_MODEL = "base"  # tiny, base, small, medium, large

# ============== 1. åŠ è½½æ¨¡å‹ ==============
print("=" * 50)
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
print("=" * 50)

# 1.1 åŠ è½½ Silero VAD
print("\n[1/3] åŠ è½½ Silero VAD...")
vad_model, utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False,
    onnx=False
)
(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
print("âœ“ Silero VAD åŠ è½½å®Œæˆ")

# 1.2 åŠ è½½ Whisper
print("\n[2/3] åŠ è½½ Whisper ASR...")
import whisper
model_path = "./whisper_models/base.pt"

whisper_model = whisper.load_model(model_path)
print(f"âœ“ Whisper ({model_path}) åŠ è½½å®Œæˆ")

# 1.3 åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
print("\n[3/3] åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹...")
import sys
sys.modules['speechbrain.integrations.nlp.flair_embeddings'] = None

from speechbrain.inference.interfaces import foreign_class

emotion_classifier = foreign_class(
    source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
    pymodule_file="custom_interface.py",
    classname="CustomEncoderWav2vec2Classifier",
    savedir="pretrained_models/emotion-recognition-wav2vec2-IEMOCAP",
    run_opts={"device": "cpu"}
)
print("âœ“ æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")

print("\n" + "=" * 50)
print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
print("=" * 50)


# ============== 2. å½•éŸ³å‡½æ•° ==============
def capture_audio(duration=RECORD_SECONDS, rate=SAMPLE_RATE):
    """å½•åˆ¶éŸ³é¢‘"""
    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=rate,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    print(f"\nğŸ¤ æ­£åœ¨å½•éŸ³ {duration} ç§’...")
    frames = []
    for _ in range(0, int(rate / CHUNK * duration)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16))
    
    print("âœ“ å½•éŸ³ç»“æŸ")
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    # å½’ä¸€åŒ–åˆ° [-1, 1]
    audio = np.hstack(frames).astype(np.float32) / 32768.0
    return torch.from_numpy(audio)


# ============== 3. Silero VAD å¤„ç† ==============
def apply_silero_vad(audio_tensor, sample_rate=SAMPLE_RATE):
    """
    ä½¿ç”¨ Silero VAD æå–æœ‰æ•ˆè¯­éŸ³æ®µ
    è¿”å›: å¤„ç†åçš„éŸ³é¢‘ tensor, è¯­éŸ³æ—¶é—´æˆ³åˆ—è¡¨
    """
    # ç¡®ä¿æ˜¯ 1D tensor
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    # è·å–è¯­éŸ³æ—¶é—´æˆ³
    speech_timestamps = get_speech_timestamps(
        audio_tensor,
        vad_model,
        sampling_rate=sample_rate,
        threshold=0.5,           # è¯­éŸ³æ£€æµ‹é˜ˆå€¼
        min_speech_duration_ms=250,  # æœ€çŸ­è¯­éŸ³æ®µ 250ms
        min_silence_duration_ms=100,  # æœ€çŸ­é™éŸ³æ®µ 100ms
        speech_pad_ms=30         # å‰åå¡«å……
    )
    
    if not speech_timestamps:
        print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼")
        return audio_tensor, []
    
    # åˆå¹¶è¯­éŸ³æ®µ
    speech_audio = collect_chunks(speech_timestamps, audio_tensor)
    
    # æ‰“å° VAD ç»“æœ
    total_speech = sum(ts['end'] - ts['start'] for ts in speech_timestamps) / sample_rate
    print(f"âœ“ VAD æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªè¯­éŸ³æ®µ")
    print(f"  æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {total_speech:.2f} ç§’")
    
    for i, ts in enumerate(speech_timestamps):
        start_sec = ts['start'] / sample_rate
        end_sec = ts['end'] / sample_rate
        print(f"  æ®µ {i+1}: {start_sec:.2f}s - {end_sec:.2f}s")
    
    return speech_audio, speech_timestamps


# ============== 4. Whisper è½¬å½• ==============
def transcribe_with_whisper(audio_tensor, sample_rate=SAMPLE_RATE):
    """
    ä½¿ç”¨ Whisper è¿›è¡Œè¯­éŸ³è½¬å½•
    è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    """
    # Whisper éœ€è¦ numpy æ•°ç»„
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    
    # ç¡®ä¿æ˜¯ float32
    audio_np = audio_np.astype(np.float32)
    
    # Whisper éœ€è¦ 16kHzï¼Œå¦‚æœä¸æ˜¯åˆ™é‡é‡‡æ ·
    if sample_rate != 16000:
        import librosa
        audio_np = librosa.resample(audio_np, orig_sr=sample_rate, target_sr=16000)
    
    # è½¬å½•
    result = whisper_model.transcribe(
        audio_np,
        language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        task="transcribe",
        fp16=False  # CPU ä½¿ç”¨ fp32
    )
    
    return result


# ============== 5. æƒ…æ„Ÿåˆ†æ ==============
def analyze_emotion(audio_tensor):
    """åˆ†æè¯­éŸ³æƒ…æ„Ÿ"""
    # ç¡®ä¿æ ¼å¼æ­£ç¡® [1, time]
    if audio_tensor.dim() == 1:
        audio_tensor = audio_tensor.unsqueeze(0)
    
    # æ¨ç†
    prob, score, index, emotion = emotion_classifier.classify_batch(audio_tensor)
    
    return {
        'probs': prob[0],
        'score': score[0].item(),
        'index': index[0].item(),
        'label': emotion[0]
    }


def analyze_audio_features(audio_tensor):
    """åˆ†æéŸ³é¢‘ç‰¹å¾ï¼ˆéŸ³é‡ã€èƒ½é‡ç­‰ï¼‰"""
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    rms = torch.sqrt(torch.mean(audio_tensor ** 2))
    loudness_db = 20 * torch.log10(rms + 1e-8)
    
    # è®¡ç®—è¿‡é›¶ç‡ï¼ˆè¯­éŸ³æ´»è·ƒåº¦æŒ‡æ ‡ï¼‰
    zero_crossings = torch.sum(torch.abs(torch.diff(torch.sign(audio_tensor)))) / 2
    zcr = zero_crossings / len(audio_tensor)
    
    return {
        'rms': rms.item(),
        'loudness_db': loudness_db.item(),
        'duration': len(audio_tensor) / SAMPLE_RATE,
        'zero_crossing_rate': zcr.item()
    }


# ============== 6. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ ==============
def analyze_text_sentiment(text):
    """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
    results = {}
    
    # SnowNLP (ä¸­æ–‡)
    try:
        from snownlp import SnowNLP
        s = SnowNLP(text)
        results['snownlp'] = {
            'score': s.sentiments,
            'sentiment': "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
        }
    except ImportError:
        results['snownlp'] = None
    
    # TextBlob (è‹±æ–‡)
    try:
        from textblob import TextBlob
        blob = TextBlob(text)
        results['textblob'] = {
            'polarity': blob.sentiment.polarity,
            'subjectivity': blob.sentiment.subjectivity,
            'sentiment': "æ­£é¢" if blob.sentiment.polarity > 0.1 else "è´Ÿé¢" if blob.sentiment.polarity < -0.1 else "ä¸­æ€§"
        }
    except ImportError:
        results['textblob'] = None
    
    return results


# ============== 7. ä¿å­˜éŸ³é¢‘ ==============
def save_audio_file(audio_tensor, filename="last_recording.wav", sample_rate=SAMPLE_RATE):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    
    try:
        import soundfile as sf
        sf.write(filename, audio_np, sample_rate)
        print(f"âœ“ éŸ³é¢‘å·²ä¿å­˜: {filename}")
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
        return False


# ============== 8. ä¸»ç¨‹åº ==============
def main():
    emotion_labels = ["neuï¼ˆä¸­æ€§ï¼‰", "angï¼ˆç”Ÿæ°”ï¼‰", "hapï¼ˆå¼€å¿ƒï¼‰", "sadï¼ˆæ‚²ä¼¤ï¼‰"]
    
    # å½•éŸ³
    raw_audio = capture_audio()
    
    # VAD å¤„ç†
    print("\n" + "=" * 50)
    print("ã€Silero VAD è¯­éŸ³æ£€æµ‹ã€‘")
    print("=" * 50)
    speech_audio, timestamps = apply_silero_vad(raw_audio)
    
    if len(timestamps) == 0:
        print("æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•ï¼")
        return
    
    # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘
    save_audio_file(speech_audio, "vad_processed.wav")
    save_audio_file(raw_audio, "raw_recording.wav")
    
    # Whisper è½¬å½•
    print("\n" + "=" * 50)
    print("ã€Whisper è¯­éŸ³è½¬å½•ã€‘")
    print("=" * 50)
    whisper_result = transcribe_with_whisper(speech_audio)
    transcript = whisper_result['text'].strip()
    detected_language = whisper_result.get('language', 'unknown')
    
    print(f"æ£€æµ‹è¯­è¨€: {detected_language}")
    print(f"è½¬å½•æ–‡æœ¬: {transcript}")
    
    # æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå£°éŸ³ï¼‰
    print("\n" + "=" * 50)
    print("ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æã€‘")
    print("=" * 50)
    
    emotion_result = analyze_emotion(speech_audio)
    audio_features = analyze_audio_features(speech_audio)
    
    print(f"\né¢„æµ‹æƒ…ç»ª: {emotion_labels[emotion_result['index']]} ({emotion_result['label']})")
    print(f"ç½®ä¿¡åº¦: {emotion_result['score']:.4f}")
    
    print("\nå„ç±»åˆ«æ¦‚ç‡ï¼š")
    for i, label in enumerate(emotion_labels):
        print(f"  {label:12} : {emotion_result['probs'][i]:.4%}")
    
    # éŸ³é¢‘ç‰¹å¾
    print("\n" + "-" * 30)
    print("ã€éŸ³é¢‘ç‰¹å¾ã€‘")
    print(f"  éŸ³é‡æ°´å¹³: {audio_features['loudness_db']:.2f} dB")
    print(f"  éŸ³é¢‘é•¿åº¦: {audio_features['duration']:.2f} ç§’")
    print(f"  RMS èƒ½é‡: {audio_features['rms']:.6f}")
    print(f"  è¿‡é›¶ç‡: {audio_features['zero_crossing_rate']:.6f}")
    
    # ç»¼åˆåˆ¤æ–­
    emotion_label = emotion_result['label']
    loudness = audio_features['loudness_db']
    
    print("\nã€ç»¼åˆåˆ†æã€‘")
    if emotion_label == 'ang' and loudness > -20:
        print("â†’ å¤§å£°ç”Ÿæ°”ï¼å¯èƒ½åœ¨äº‰åµ")
    elif emotion_label == 'hap' and loudness > -25:
        print("â†’ å…´å¥‹å¼€å¿ƒï¼Œå¤§å£°è¡¨è¾¾")
    elif emotion_label == 'sad' and loudness < -35:
        print("â†’ ä½å£°ä¼¤å¿ƒï¼Œå¾ˆä½è½")
    else:
        print("â†’ æƒ…ç»ªæ­£å¸¸éŸ³é‡")
    
    max_prob = emotion_result['probs'].max().item()
    if max_prob > 0.8:
        print("â†’ æƒ…ç»ªåˆ¤æ–­å¾ˆæ˜ç¡®")
    elif max_prob > 0.6:
        print("â†’ æƒ…ç»ªæ¯”è¾ƒæ˜æ˜¾ï¼Œä½†æœ‰ä¸€å®šä¸ç¡®å®šæ€§")
    else:
        print("â†’ æƒ…ç»ªè¾ƒæ¨¡ç³Šï¼Œå¯èƒ½ä¸ºä¸­æ€§æˆ–æ··åˆæƒ…ç»ª")
    
    # æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
    if transcript:
        print("\n" + "=" * 50)
        print("ã€æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€‘")
        print("=" * 50)
        
        text_sentiment = analyze_text_sentiment(transcript)
        
        if text_sentiment.get('snownlp'):
            print(f"\nSnowNLP (ä¸­æ–‡):")
            print(f"  æƒ…æ„Ÿå¾—åˆ†: {text_sentiment['snownlp']['score']:.4f}")
            print(f"  åˆ¤æ–­ç»“æœ: {text_sentiment['snownlp']['sentiment']}")
        
        if text_sentiment.get('textblob'):
            print(f"\nTextBlob (è‹±æ–‡):")
            print(f"  ææ€§: {text_sentiment['textblob']['polarity']:.4f}")
            print(f"  ä¸»è§‚æ€§: {text_sentiment['textblob']['subjectivity']:.4f}")
            print(f"  åˆ¤æ–­ç»“æœ: {text_sentiment['textblob']['sentiment']}")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 50)
    print("ã€åˆ†ææ€»ç»“ã€‘")
    print("=" * 50)
    print(f"  è½¬å½•å†…å®¹: {transcript}")
    print(f"  è¯­éŸ³æƒ…ç»ª: {emotion_labels[emotion_result['index']]}")
    print(f"  è¯­éŸ³ç½®ä¿¡åº¦: {emotion_result['score']:.2%}")
    if transcript and text_sentiment.get('snownlp'):
        print(f"  æ–‡æœ¬æƒ…æ„Ÿ: {text_sentiment['snownlp']['sentiment']} ({text_sentiment['snownlp']['score']:.2%})")
    print("=" * 50)


if __name__ == "__main__":
    main()
