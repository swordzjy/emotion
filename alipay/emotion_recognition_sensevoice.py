"""
è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç¨‹åº v4.1 - SenseVoice ç‰ˆæœ¬
ä½¿ç”¨é˜¿é‡Œ SenseVoice æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒï¼š
- è¯­éŸ³è¯†åˆ«ï¼ˆä¸­è‹±æ—¥éŸ©ç²¤ 50+ è¯­è¨€ï¼‰
- æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¼€å¿ƒ/æ‚²ä¼¤/ç”Ÿæ°”/ä¸­æ€§/æƒŠè®¶ï¼‰
- è¯­éŸ³äº‹ä»¶æ£€æµ‹ï¼ˆç¬‘å£°ã€æŒå£°ã€å’³å—½ç­‰ï¼‰

SenseVoice æ˜¯ä¸€ç«™å¼æ–¹æ¡ˆï¼Œæ— éœ€å•ç‹¬åŠ è½½æƒ…æ„Ÿæ¨¡å‹ï¼
"""

import pyaudio
import os
import numpy as np
import torch
import warnings
warnings.filterwarnings("ignore")

# ============== é…ç½®å‚æ•° ==============
SAMPLE_RATE = 16000
RECORD_SECONDS = 8
CHUNK = 512
# ============== é…ç½® ffmpeg è·¯å¾„ ==============
FFMPEG_PATH = r"E:\Python\ffmpeg\bin"  # â† ç¡®è®¤è¿™ä¸ªè·¯å¾„

if os.path.exists(FFMPEG_PATH):
    os.environ["PATH"] = FFMPEG_PATH + os.pathsep + os.environ.get("PATH", "")

# ============== 1. åŠ è½½æ¨¡å‹ ==============
print("=" * 50)
print("æ­£åœ¨åŠ è½½ SenseVoice æ¨¡å‹...")
print("ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½çº¦ 900MBï¼‰")
print("=" * 50)
SILERO_VAD_PATH = r"E:\Workspace\claude\emotion\silero-vad"  # â† æœ¬åœ° Silero VAD ä»“åº“è·¯å¾„
# 1.1 åŠ è½½ Silero VAD
print("\n[1/2] åŠ è½½ Silero VAD...")
# å¦‚æœæœ¬åœ°ç›®å½•å­˜åœ¨ï¼Œä»æœ¬åœ°åŠ è½½ï¼›å¦åˆ™ä»ç½‘ç»œä¸‹è½½
if os.path.exists(SILERO_VAD_PATH):
    print(f"   ä»æœ¬åœ°åŠ è½½: {SILERO_VAD_PATH}")
    vad_model, utils = torch.hub.load(
        repo_or_dir=SILERO_VAD_PATH,
        model='silero_vad',
        source='local',  # å…³é”®ï¼šæŒ‡å®šæœ¬åœ°æº
        onnx=False
    )
else:
    print("   æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»ç½‘ç»œä¸‹è½½...")
    vad_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )

(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
print("âœ“ Silero VAD åŠ è½½å®Œæˆ")

# 1.2 åŠ è½½ SenseVoice
print("\n[2/2] åŠ è½½ SenseVoice...")
from funasr import AutoModel

# SenseVoice-Small: å¤šè¯­è¨€ + æƒ…æ„Ÿ + äº‹ä»¶æ£€æµ‹
sensevoice_model = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    disable_update=True,
    device="cpu"
)
print("âœ“ SenseVoice åŠ è½½å®Œæˆ")
print("  æ”¯æŒ: è¯­éŸ³è¯†åˆ« + æƒ…æ„Ÿè¯†åˆ« + è¯­éŸ³äº‹ä»¶æ£€æµ‹")

print("\n" + "=" * 50)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
print("=" * 50)


# ============== 2. å½•éŸ³å‡½æ•° ==============
def capture_audio(duration=RECORD_SECONDS, rate=SAMPLE_RATE):
    """å½•åˆ¶éŸ³é¢‘"""
    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=rate,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    print(f"\nğŸ¤ æ­£åœ¨å½•éŸ³ {duration} ç§’...")
    frames = []
    for i in range(0, int(rate / CHUNK * duration)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16))
        # è¿›åº¦æ˜¾ç¤º
        if i % 30 == 0:
            print(f"\r   å½•éŸ³ä¸­: {i * CHUNK / rate:.1f}s / {duration}s", end="")
    
    print(f"\râœ“ å½•éŸ³ç»“æŸ: {duration}s                    ")
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    audio = np.hstack(frames).astype(np.float32) / 32768.0
    return torch.from_numpy(audio)


# ============== 3. Silero VAD å¤„ç† ==============
def apply_silero_vad(audio_tensor, sample_rate=SAMPLE_RATE):
    """ä½¿ç”¨ Silero VAD æå–æœ‰æ•ˆè¯­éŸ³æ®µ"""
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    speech_timestamps = get_speech_timestamps(
        audio_tensor,
        vad_model,
        sampling_rate=sample_rate,
        threshold=0.5,
        min_speech_duration_ms=250,
        min_silence_duration_ms=100,
        speech_pad_ms=30
    )
    
    if not speech_timestamps:
        print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼")
        return audio_tensor, []
    
    speech_audio = collect_chunks(speech_timestamps, audio_tensor)
    
    total_speech = sum(ts['end'] - ts['start'] for ts in speech_timestamps) / sample_rate
    print(f"âœ“ VAD æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªè¯­éŸ³æ®µ")
    print(f"  æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {total_speech:.2f} ç§’")
    
    return speech_audio, speech_timestamps


# ============== 4. SenseVoice è¯†åˆ« ==============
def recognize_with_sensevoice(audio_tensor, sample_rate=SAMPLE_RATE):
    """
    ä½¿ç”¨ SenseVoice è¿›è¡Œè¯†åˆ«
    è¿”å›: æ–‡æœ¬ã€è¯­è¨€ã€æƒ…æ„Ÿã€äº‹ä»¶
    """
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    
    # SenseVoice è¯†åˆ«
    result = sensevoice_model.generate(
        input=audio_np,
        cache={},
        language="auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼Œæˆ– "zh", "en", "ja", "ko", "yue"
        use_itn=True,     # é€†æ–‡æœ¬å½’ä¸€åŒ–ï¼ˆæ•°å­—ã€æ—¥æœŸç­‰è½¬æ¢ï¼‰
        batch_size_s=60,
    )
    
    if not result or len(result) == 0:
        return {
            'text': '',
            'language': 'unknown',
            'emotion': 'unknown',
            'event': None,
            'raw': None
        }
    
    raw_text = result[0].get('text', '')
    
    # è§£æ SenseVoice è¾“å‡ºæ ¼å¼: <|è¯­è¨€|><|æƒ…æ„Ÿ|><|äº‹ä»¶|>æ–‡æœ¬å†…å®¹
    # ä¾‹å¦‚: <|zh|><|HAPPY|><|Speech|>ä»Šå¤©å¤©æ°”çœŸå¥½
    parsed = parse_sensevoice_output(raw_text)
    
    return parsed


def parse_sensevoice_output(text):
    """
    è§£æ SenseVoice è¾“å‡º
    æ ¼å¼: <|lang|><|emotion|><|event|>å®é™…æ–‡æœ¬
    """
    import re
    
    # æƒ…æ„Ÿæ˜ å°„
    emotion_map = {
        'HAPPY': 'å¼€å¿ƒ ğŸ˜Š',
        'SAD': 'æ‚²ä¼¤ ğŸ˜¢',
        'ANGRY': 'ç”Ÿæ°” ğŸ˜ ',
        'NEUTRAL': 'ä¸­æ€§ ğŸ˜',
        'SURPRISE': 'æƒŠè®¶ ğŸ˜²',
        'FEARFUL': 'ææƒ§ ğŸ˜°',
        'DISGUSTED': 'åŒæ¶ ğŸ¤¢',
    }
    
    # äº‹ä»¶æ˜ å°„
    event_map = {
        'Speech': 'è¯­éŸ³',
        'Laughter': 'ç¬‘å£° ğŸ˜„',
        'Applause': 'æŒå£° ğŸ‘',
        'Cry': 'å“­æ³£ ğŸ˜­',
        'Cough': 'å’³å—½',
        'Sneeze': 'å–·åš',
        'Music': 'éŸ³ä¹ ğŸµ',
        'BGM': 'èƒŒæ™¯éŸ³ä¹',
    }
    
    # æå–æ ‡ç­¾
    pattern = r'<\|([^|]+)\|>'
    tags = re.findall(pattern, text)
    
    # ç§»é™¤æ ‡ç­¾è·å–çº¯æ–‡æœ¬
    clean_text = re.sub(pattern, '', text).strip()
    
    # è§£æ
    language = tags[0] if len(tags) > 0 else 'unknown'
    emotion_raw = tags[1] if len(tags) > 1 else 'NEUTRAL'
    event_raw = tags[2] if len(tags) > 2 else 'Speech'
    
    # è¯­è¨€æ˜ å°„
    lang_map = {
        'zh': 'ä¸­æ–‡',
        'en': 'è‹±æ–‡',
        'ja': 'æ—¥è¯­',
        'ko': 'éŸ©è¯­',
        'yue': 'ç²¤è¯­',
    }
    
    return {
        'text': clean_text,
        'language': lang_map.get(language, language),
        'language_code': language,
        'emotion': emotion_map.get(emotion_raw, emotion_raw),
        'emotion_raw': emotion_raw,
        'event': event_map.get(event_raw, event_raw),
        'event_raw': event_raw,
        'raw': text
    }


# ============== 5. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ ==============
def analyze_text_sentiment(text):
    """åˆ†ææ–‡æœ¬æƒ…æ„Ÿï¼ˆä½œä¸ºè¡¥å……ï¼‰"""
    try:
        from snownlp import SnowNLP
        s = SnowNLP(text)
        return {
            'score': s.sentiments,
            'sentiment': "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
        }
    except:
        return None


# ============== 6. éŸ³é¢‘ç‰¹å¾ ==============
def analyze_audio_features(audio_tensor):
    """åˆ†æéŸ³é¢‘ç‰¹å¾"""
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    rms = torch.sqrt(torch.mean(audio_tensor ** 2))
    loudness_db = 20 * torch.log10(rms + 1e-8)
    
    return {
        'loudness_db': loudness_db.item(),
        'duration': len(audio_tensor) / SAMPLE_RATE,
    }


# ============== 7. ä¿å­˜éŸ³é¢‘ ==============
def save_audio_file(audio_tensor, filename="recording.wav"):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
    try:
        import soundfile as sf
        audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
        sf.write(filename, audio_np, SAMPLE_RATE)
        print(f"âœ“ éŸ³é¢‘å·²ä¿å­˜: {filename}")
    except:
        pass


# ============== 8. ä¸»ç¨‹åº ==============
def main():
    # å½•éŸ³
    raw_audio = capture_audio()
    
    # VAD å¤„ç†
    print("\n" + "=" * 50)
    print("ã€Silero VAD è¯­éŸ³æ£€æµ‹ã€‘")
    print("=" * 50)
    speech_audio, timestamps = apply_silero_vad(raw_audio)
    
    if len(timestamps) == 0:
        print("æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•ï¼")
        return
    
    save_audio_file(speech_audio)
    
    # SenseVoice è¯†åˆ«
    print("\n" + "=" * 50)
    print("ã€SenseVoice è¯­éŸ³è¯†åˆ« + æƒ…æ„Ÿåˆ†æã€‘")
    print("=" * 50)
    
    result = recognize_with_sensevoice(speech_audio)
    print( f"âœ“ è¯†åˆ«å®Œæˆï¼:{result}")
    print(f"\nğŸ“ è¯†åˆ«æ–‡æœ¬: {result['text']}")
    print(f"ğŸŒ è¯†åˆ«è¯­è¨€: {result['language']}")
    print(f"ğŸ­ è¯­éŸ³æƒ…æ„Ÿ: {result['emotion']}")
    print(f"ğŸ”Š è¯­éŸ³äº‹ä»¶: {result['event']}")
    
    # éŸ³é¢‘ç‰¹å¾
    features = analyze_audio_features(speech_audio)
    print(f"\nğŸ“Š éŸ³é‡: {features['loudness_db']:.1f} dB | æ—¶é•¿: {features['duration']:.2f}s")
    
    # æ–‡æœ¬æƒ…æ„Ÿåˆ†æï¼ˆè¡¥å……ï¼‰
    if result['text']:
        print("\n" + "-" * 30)
        print("ã€æ–‡æœ¬æƒ…æ„Ÿåˆ†æ (SnowNLP)ã€‘")
        text_sent = analyze_text_sentiment(result['text'])
        if text_sent:
            print(f"   å¾—åˆ†: {text_sent['score']:.2%} â†’ {text_sent['sentiment']}")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("=" * 50)
    print(f"  ğŸ“ å†…å®¹: {result['text']}")
    print(f"  ğŸŒ è¯­è¨€: {result['language']}")
    print(f"  ğŸ­ æƒ…æ„Ÿ: {result['emotion']}")
    print(f"  ğŸ”Š äº‹ä»¶: {result['event']}")
    
    # ç»¼åˆåˆ¤æ–­
    print("\nğŸ’¡ ç»¼åˆåˆ¤æ–­:")
    emotion_raw = result.get('emotion_raw', 'NEUTRAL')
    loudness = features['loudness_db']
    
    if emotion_raw == 'ANGRY' and loudness > -20:
        print("   â†’ ğŸ”¥ å¤§å£°ç”Ÿæ°”ï¼Œæƒ…ç»ªæ¿€åŠ¨ï¼")
    elif emotion_raw == 'HAPPY' and loudness > -25:
        print("   â†’ ğŸ‰ å…´å¥‹å¼€å¿ƒï¼Œæƒ…ç»ªé«˜æ¶¨ï¼")
    elif emotion_raw == 'SAD' and loudness < -30:
        print("   â†’ ğŸ’§ ä½å£°æ‚²ä¼¤ï¼Œæƒ…ç»ªä½è½")
    elif emotion_raw == 'SURPRISE':
        print("   â†’ â— è¯­æ°”æƒŠè®¶ï¼Œå¯èƒ½æœ‰æ„å¤–å‘ç°")
    else:
        print("   â†’ ğŸ˜Œ æƒ…ç»ªå¹³ç¨³")


if __name__ == "__main__":
    main()
