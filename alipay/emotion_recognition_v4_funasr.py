"""
è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç¨‹åº v4.0
ä½¿ç”¨ Silero VAD + FunASR Paraformerï¼ˆä¸­æ–‡è¯†åˆ«æ•ˆæœæœ€ä½³ï¼‰

FunASR æ˜¯é˜¿é‡Œè¾¾æ‘©é™¢å¼€æºçš„è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ï¼ŒParaformer æ¨¡å‹åœ¨ä¸­æ–‡ä¸Šæ•ˆæœè¿œè¶… Whisper
"""

import pyaudio
import numpy as np
import torch
import warnings
warnings.filterwarnings("ignore")

# ============== é…ç½®å‚æ•° ==============
SAMPLE_RATE = 16000
RECORD_SECONDS = 8
CHUNK = 512

# FunASR æ¨¡å‹é€‰é¡¹:
# - "paraformer-zh"          : ä¸­æ–‡ï¼Œæ•ˆæœæœ€å¥½ï¼Œæ¨è
# - "paraformer-zh-streaming": ä¸­æ–‡æµå¼
# - "sensevoice-small"       : å¤šè¯­è¨€ + æƒ…æ„Ÿè¯†åˆ«
ASR_MODEL = "paraformer-zh"

# ============== 1. åŠ è½½æ¨¡å‹ ==============
print("=" * 50)
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
print("=" * 50)

# 1.1 åŠ è½½ Silero VAD
print("\n[1/3] åŠ è½½ Silero VAD...")
vad_model, utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False,
    onnx=False
)
(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
print("âœ“ Silero VAD åŠ è½½å®Œæˆ")

# 1.2 åŠ è½½ FunASR
print("\n[2/3] åŠ è½½ FunASR ASR æ¨¡å‹...")
try:
    from funasr import AutoModel
    
    # Paraformer-zh: ä¸­æ–‡æ•ˆæœæœ€å¥½çš„å¼€æºæ¨¡å‹
    asr_model = AutoModel(
        model="paraformer-zh",  # æˆ– "iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
        vad_model="fsmn-vad",   # FunASR è‡ªå¸¦çš„ VADï¼ˆå¯é€‰ï¼Œæˆ‘ä»¬å·²æœ‰ Sileroï¼‰
        punc_model="ct-punc",   # æ ‡ç‚¹æ¢å¤æ¨¡å‹
        device="cpu"
    )
    print(f"âœ“ FunASR Paraformer åŠ è½½å®Œæˆ")
    ASR_TYPE = "funasr"
    
except ImportError:
    print("âš ï¸ FunASR æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ faster-whisper...")
    try:
        from faster_whisper import WhisperModel
        asr_model = WhisperModel("large-v3", device="cpu", compute_type="int8")
        print("âœ“ Faster-Whisper large-v3 åŠ è½½å®Œæˆ")
        ASR_TYPE = "whisper"
    except:
        print("âŒ è¯·å®‰è£… FunASR: pip install funasr modelscope")
        exit(1)

# 1.3 åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
print("\n[3/3] åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹...")
import sys
sys.modules['speechbrain.integrations.nlp.flair_embeddings'] = None

from speechbrain.inference.interfaces import foreign_class

emotion_classifier = foreign_class(
    source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
    pymodule_file="custom_interface.py",
    classname="CustomEncoderWav2vec2Classifier",
    savedir="pretrained_models/emotion-recognition-wav2vec2-IEMOCAP",
    run_opts={"device": "cpu"}
)
print("âœ“ æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")

print("\n" + "=" * 50)
print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
print("=" * 50)


# ============== 2. å½•éŸ³å‡½æ•° ==============
def capture_audio(duration=RECORD_SECONDS, rate=SAMPLE_RATE):
    """å½•åˆ¶éŸ³é¢‘"""
    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=rate,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    print(f"\nğŸ¤ æ­£åœ¨å½•éŸ³ {duration} ç§’...")
    frames = []
    for _ in range(0, int(rate / CHUNK * duration)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16))
    
    print("âœ“ å½•éŸ³ç»“æŸ")
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    audio = np.hstack(frames).astype(np.float32) / 32768.0
    return torch.from_numpy(audio)


# ============== 3. Silero VAD å¤„ç† ==============
def apply_silero_vad(audio_tensor, sample_rate=SAMPLE_RATE):
    """ä½¿ç”¨ Silero VAD æå–æœ‰æ•ˆè¯­éŸ³æ®µ"""
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    speech_timestamps = get_speech_timestamps(
        audio_tensor,
        vad_model,
        sampling_rate=sample_rate,
        threshold=0.5,
        min_speech_duration_ms=250,
        min_silence_duration_ms=100,
        speech_pad_ms=30
    )
    
    if not speech_timestamps:
        print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼")
        return audio_tensor, []
    
    speech_audio = collect_chunks(speech_timestamps, audio_tensor)
    
    total_speech = sum(ts['end'] - ts['start'] for ts in speech_timestamps) / sample_rate
    print(f"âœ“ VAD æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªè¯­éŸ³æ®µ")
    print(f"  æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {total_speech:.2f} ç§’")
    
    return speech_audio, speech_timestamps


# ============== 4. ASR è½¬å½• ==============
def transcribe_audio(audio_tensor, sample_rate=SAMPLE_RATE):
    """è¯­éŸ³è½¬æ–‡å­—"""
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    
    if ASR_TYPE == "funasr":
        # FunASR Paraformer
        result = asr_model.generate(
            input=audio_np,
            batch_size_s=300,  # æ‰¹å¤„ç†å¤§å°ï¼ˆç§’ï¼‰
        )
        
        # æå–æ–‡æœ¬
        if result and len(result) > 0:
            text = result[0].get('text', '')
            return {
                'text': text,
                'language': 'zh',
                'model': 'FunASR Paraformer'
            }
        return {'text': '', 'language': 'zh', 'model': 'FunASR Paraformer'}
    
    else:
        # Whisper fallback
        segments, info = asr_model.transcribe(audio_np, language="zh")
        text = " ".join([seg.text for seg in segments])
        return {
            'text': text.strip(),
            'language': info.language,
            'model': 'Whisper large-v3'
        }


# ============== 5. æƒ…æ„Ÿåˆ†æ ==============
def analyze_emotion(audio_tensor):
    """åˆ†æè¯­éŸ³æƒ…æ„Ÿ"""
    if audio_tensor.dim() == 1:
        audio_tensor = audio_tensor.unsqueeze(0)
    
    prob, score, index, emotion = emotion_classifier.classify_batch(audio_tensor)
    
    return {
        'probs': prob[0],
        'score': score[0].item(),
        'index': index[0].item(),
        'label': emotion[0]
    }


def analyze_audio_features(audio_tensor):
    """åˆ†æéŸ³é¢‘ç‰¹å¾"""
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    rms = torch.sqrt(torch.mean(audio_tensor ** 2))
    loudness_db = 20 * torch.log10(rms + 1e-8)
    
    return {
        'rms': rms.item(),
        'loudness_db': loudness_db.item(),
        'duration': len(audio_tensor) / SAMPLE_RATE,
    }


# ============== 6. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ ==============
def analyze_text_sentiment(text):
    """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
    results = {}
    
    try:
        from snownlp import SnowNLP
        s = SnowNLP(text)
        results['snownlp'] = {
            'score': s.sentiments,
            'sentiment': "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
        }
    except ImportError:
        pass
    
    return results


# ============== 7. ä¿å­˜éŸ³é¢‘ ==============
def save_audio_file(audio_tensor, filename="recording.wav", sample_rate=SAMPLE_RATE):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    try:
        import soundfile as sf
        sf.write(filename, audio_np, sample_rate)
        print(f"âœ“ éŸ³é¢‘å·²ä¿å­˜: {filename}")
    except:
        pass


# ============== 8. ä¸»ç¨‹åº ==============
def main():
    emotion_labels = ["neuï¼ˆä¸­æ€§ï¼‰", "angï¼ˆç”Ÿæ°”ï¼‰", "hapï¼ˆå¼€å¿ƒï¼‰", "sadï¼ˆæ‚²ä¼¤ï¼‰"]
    
    # å½•éŸ³
    raw_audio = capture_audio()
    
    # VAD å¤„ç†
    print("\n" + "=" * 50)
    print("ã€Silero VAD è¯­éŸ³æ£€æµ‹ã€‘")
    print("=" * 50)
    speech_audio, timestamps = apply_silero_vad(raw_audio)
    
    if len(timestamps) == 0:
        print("æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•ï¼")
        return
    
    save_audio_file(speech_audio, "vad_processed.wav")
    
    # ASR è½¬å½•
    print("\n" + "=" * 50)
    print(f"ã€è¯­éŸ³è½¬å½• - {ASR_TYPE.upper()}ã€‘")
    print("=" * 50)
    
    asr_result = transcribe_audio(speech_audio)
    transcript = asr_result['text'].strip()
    
    print(f"ä½¿ç”¨æ¨¡å‹: {asr_result.get('model', ASR_TYPE)}")
    print(f"è¯†åˆ«è¯­è¨€: {asr_result.get('language', 'zh')}")
    print(f"è½¬å½•æ–‡æœ¬: {transcript}")
    
    # æƒ…æ„Ÿåˆ†æ
    print("\n" + "=" * 50)
    print("ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æã€‘")
    print("=" * 50)
    
    emotion_result = analyze_emotion(speech_audio)
    audio_features = analyze_audio_features(speech_audio)
    
    print(f"\né¢„æµ‹æƒ…ç»ª: {emotion_labels[emotion_result['index']]} ({emotion_result['label']})")
    print(f"ç½®ä¿¡åº¦: {emotion_result['score']:.4f}")
    
    print("\nå„ç±»åˆ«æ¦‚ç‡ï¼š")
    for i, label in enumerate(emotion_labels):
        bar = "â–ˆ" * int(emotion_result['probs'][i] * 20)
        print(f"  {label:12} : {bar} {emotion_result['probs'][i]:.2%}")
    
    # éŸ³é¢‘ç‰¹å¾
    print(f"\néŸ³é‡: {audio_features['loudness_db']:.1f} dB | æ—¶é•¿: {audio_features['duration']:.2f}s")
    
    # æ–‡æœ¬æƒ…æ„Ÿ
    if transcript:
        print("\n" + "=" * 50)
        print("ã€æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€‘")
        print("=" * 50)
        
        text_sentiment = analyze_text_sentiment(transcript)
        if text_sentiment.get('snownlp'):
            s = text_sentiment['snownlp']
            print(f"SnowNLP: {s['score']:.2%} â†’ {s['sentiment']}")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("=" * 50)
    print(f"  ğŸ“ å†…å®¹: {transcript}")
    print(f"  ğŸ­ è¯­éŸ³æƒ…ç»ª: {emotion_labels[emotion_result['index']]}")
    print(f"  ğŸ“ˆ ç½®ä¿¡åº¦: {emotion_result['score']:.0%}")


if __name__ == "__main__":
    main()
