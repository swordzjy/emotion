"""
è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç¨‹åº v5.0 - å‘½ä»¤è¡Œç‰ˆæœ¬
æ”¯æŒåˆ‡æ¢ä¸¤ç§æ¨¡å¼ï¼š
  1. paraformer - Paraformer(ASR) + SpeechBrain(æƒ…æ„Ÿ) ç»„åˆ
  2. sensevoice - SenseVoice ä¸€ç«™å¼ï¼ˆASR + æƒ…æ„Ÿ + äº‹ä»¶æ£€æµ‹ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
  python emotion_recognition_cli.py                  # äº¤äº’å¼é€‰æ‹©
  python emotion_recognition_cli.py -m paraformer   # ç›´æ¥ä½¿ç”¨ Paraformer
  python emotion_recognition_cli.py -m sensevoice   # ç›´æ¥ä½¿ç”¨ SenseVoice
  python emotion_recognition_cli.py -m both         # ä¸¤ä¸ªéƒ½æµ‹è¯•å¯¹æ¯”
"""

import os
import sys
import argparse

# ============== é…ç½® ==============
FFMPEG_PATH = r"E:\Python\ffmpeg\bin"
if os.path.exists(FFMPEG_PATH):
    os.environ["PATH"] = FFMPEG_PATH + os.pathsep + os.environ.get("PATH", "")

SAMPLE_RATE = 16000
RECORD_SECONDS = 8
CHUNK = 512

# ============== å¯¼å…¥åº“ ==============
import pyaudio
import numpy as np
import torch
import warnings
warnings.filterwarnings("ignore")


# ============== å…¨å±€å˜é‡ ==============
vad_model = None
vad_utils = None
paraformer_model = None
sensevoice_model = None
emotion_classifier = None


# ============== æ¨¡å‹åŠ è½½å‡½æ•° ==============
def load_silero_vad():
    """åŠ è½½ Silero VAD"""
    global vad_model, vad_utils
    
    if vad_model is not None:
        return
    
    print("\n[VAD] åŠ è½½ Silero VAD...")
    
    # æœ¬åœ°è·¯å¾„
    local_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "silero-vad")
    
    if os.path.exists(local_path):
        vad_model, vad_utils = torch.hub.load(
            repo_or_dir=local_path,
            model='silero_vad',
            source='local',
            onnx=False
        )
        print(f"      âœ“ æœ¬åœ°åŠ è½½")
    else:
        vad_model, vad_utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
            onnx=False
        )
        print(f"      âœ“ ç½‘ç»œåŠ è½½")


def load_paraformer():
    """åŠ è½½ Paraformer + SpeechBrain"""
    global paraformer_model, emotion_classifier
    
    if paraformer_model is not None:
        return
    
    print("\n[ASR] åŠ è½½ Paraformer...")
    from funasr import AutoModel
    
    paraformer_model = AutoModel(
        model="paraformer-zh",
        vad_model="fsmn-vad",
        punc_model="ct-punc",
        device="cpu"
    )
    print("      âœ“ Paraformer åŠ è½½å®Œæˆ")
    
    # åŠ è½½æƒ…æ„Ÿæ¨¡å‹
    if emotion_classifier is None:
        print("\n[EMO] åŠ è½½ SpeechBrain æƒ…æ„Ÿæ¨¡å‹...")
        sys.modules['speechbrain.integrations.nlp.flair_embeddings'] = None
        from speechbrain.inference.interfaces import foreign_class
        
        emotion_classifier = foreign_class(
            source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
            pymodule_file="custom_interface.py",
            classname="CustomEncoderWav2vec2Classifier",
            savedir="pretrained_models/emotion-recognition-wav2vec2-IEMOCAP",
            run_opts={"device": "cpu"}
        )
        print("      âœ“ SpeechBrain æƒ…æ„Ÿæ¨¡å‹åŠ è½½å®Œæˆ")


def load_sensevoice():
    """åŠ è½½ SenseVoice"""
    global sensevoice_model
    
    if sensevoice_model is not None:
        return
    
    print("\n[ASR+EMO] åŠ è½½ SenseVoice...")
    from funasr import AutoModel
    
    # å°è¯•å¤šç§åŠ è½½æ–¹å¼
    load_methods = [
        {"model": "iic/SenseVoiceSmall", "trust_remote_code": True},
        {"model": "FunAudioLLM/SenseVoiceSmall", "trust_remote_code": True},
        {"model": "iic/SenseVoiceSmall", "trust_remote_code": True, "model_revision": "master"},
    ]
    
    for i, kwargs in enumerate(load_methods):
        try:
            sensevoice_model = AutoModel(**kwargs, device="cpu")
            print(f"      âœ“ SenseVoice åŠ è½½å®Œæˆ (æ–¹æ³• {i+1})")
            return
        except Exception as e:
            print(f"      æ–¹æ³• {i+1} å¤±è´¥: {e}")
    
    raise RuntimeError("SenseVoice åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ funasr ç‰ˆæœ¬")


# ============== éŸ³é¢‘å¤„ç†å‡½æ•° ==============
def capture_audio(duration=RECORD_SECONDS):
    """å½•éŸ³"""
    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    print(f"\nğŸ¤ å¼€å§‹å½•éŸ³ ({duration}ç§’)...")
    frames = []
    total_chunks = int(SAMPLE_RATE / CHUNK * duration)
    
    for i in range(total_chunks):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16))
        
        # è¿›åº¦æ¡
        progress = int((i + 1) / total_chunks * 30)
        bar = "â–ˆ" * progress + "â–‘" * (30 - progress)
        print(f"\r   [{bar}] {(i+1)*CHUNK/SAMPLE_RATE:.1f}s", end="")
    
    print(f"\r   âœ“ å½•éŸ³å®Œæˆ ({duration}s)                    ")
    
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    audio = np.hstack(frames).astype(np.float32) / 32768.0
    return torch.from_numpy(audio)


def apply_vad(audio_tensor):
    """VAD å¤„ç†"""
    get_speech_timestamps = vad_utils[0]
    collect_chunks = vad_utils[4]
    
    if audio_tensor.dim() > 1:
        audio_tensor = audio_tensor.squeeze()
    
    timestamps = get_speech_timestamps(
        audio_tensor,
        vad_model,
        sampling_rate=SAMPLE_RATE,
        threshold=0.5,
        min_speech_duration_ms=250,
        min_silence_duration_ms=100
    )
    
    if not timestamps:
        print("   âš ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³")
        return audio_tensor, []
    
    speech_audio = collect_chunks(timestamps, audio_tensor)
    duration = sum(t['end'] - t['start'] for t in timestamps) / SAMPLE_RATE
    print(f"   âœ“ VAD: {len(timestamps)} æ®µè¯­éŸ³, å…± {duration:.2f}s")
    
    return speech_audio, timestamps


def save_audio(audio_tensor, filename="recording.wav"):
    """ä¿å­˜éŸ³é¢‘"""
    try:
        import soundfile as sf
        audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
        sf.write(filename, audio_np, SAMPLE_RATE)
        print(f"   âœ“ å·²ä¿å­˜: {filename}")
    except Exception as e:
        print(f"   âš ï¸ ä¿å­˜å¤±è´¥: {e}")


# ============== Paraformer æ¨¡å¼ ==============
def run_paraformer(audio_tensor):
    """Paraformer + SpeechBrain æ¨¡å¼"""
    print("\n" + "=" * 50)
    print("ã€æ¨¡å¼: Paraformer + SpeechBrainã€‘")
    print("=" * 50)
    
    # ASR è½¬å½•
    print("\nğŸ“ è¯­éŸ³è¯†åˆ« (Paraformer)...")
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    result = paraformer_model.generate(input=audio_np, batch_size_s=300)
    
    transcript = ""
    if result and len(result) > 0:
        transcript = result[0].get('text', '')
    
    print(f"   è½¬å½•: {transcript}")
    
    # æƒ…æ„Ÿåˆ†æ
    print("\nğŸ­ æƒ…æ„Ÿåˆ†æ (SpeechBrain)...")
    emotion_labels = ["ä¸­æ€§ ğŸ˜", "ç”Ÿæ°” ğŸ˜ ", "å¼€å¿ƒ ğŸ˜Š", "æ‚²ä¼¤ ğŸ˜¢"]
    
    audio_input = audio_tensor.unsqueeze(0) if audio_tensor.dim() == 1 else audio_tensor
    prob, score, index, emotion = emotion_classifier.classify_batch(audio_input)
    
    pred_idx = index[0].item()
    pred_score = score[0].item()
    
    print(f"   æƒ…ç»ª: {emotion_labels[pred_idx]} ({emotion[0]})")
    print(f"   ç½®ä¿¡åº¦: {pred_score:.2%}")
    
    print("\n   æ¦‚ç‡åˆ†å¸ƒ:")
    for i, label in enumerate(emotion_labels):
        bar_len = int(prob[0][i] * 25)
        bar = "â–ˆ" * bar_len + "â–‘" * (25 - bar_len)
        print(f"   {label[:5]:6} [{bar}] {prob[0][i]:.1%}")
    
    # æ–‡æœ¬æƒ…æ„Ÿ
    text_sentiment = None
    if transcript:
        try:
            from snownlp import SnowNLP
            s = SnowNLP(transcript)
            text_sentiment = {
                'score': s.sentiments,
                'label': "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
            }
            print(f"\nğŸ“Š æ–‡æœ¬æƒ…æ„Ÿ (SnowNLP): {text_sentiment['score']:.1%} â†’ {text_sentiment['label']}")
        except:
            pass
    
    return {
        'mode': 'Paraformer + SpeechBrain',
        'transcript': transcript,
        'emotion': emotion_labels[pred_idx],
        'emotion_raw': emotion[0],
        'confidence': pred_score,
        'text_sentiment': text_sentiment
    }


# ============== SenseVoice æ¨¡å¼ ==============
def run_sensevoice(audio_tensor):
    """SenseVoice ä¸€ç«™å¼æ¨¡å¼"""
    print("\n" + "=" * 50)
    print("ã€æ¨¡å¼: SenseVoice ä¸€ç«™å¼ã€‘")
    print("=" * 50)
    
    # SenseVoice è¯†åˆ«
    print("\nğŸ“ğŸ­ è¯­éŸ³è¯†åˆ« + æƒ…æ„Ÿåˆ†æ (SenseVoice)...")
    audio_np = audio_tensor.numpy() if isinstance(audio_tensor, torch.Tensor) else audio_tensor
    
    result = sensevoice_model.generate(
        input=audio_np,
        cache={},
        language="auto",
        use_itn=True,
        batch_size_s=60
    )
    
    if not result or len(result) == 0:
        print("   âš ï¸ è¯†åˆ«å¤±è´¥")
        return None
    
    raw_text = result[0].get('text', '')
    
    # è§£æè¾“å‡º: <|lang|><|emotion|><|event|>text
    import re
    
    # æƒ…æ„Ÿæ˜ å°„
    emotion_map = {
        'HAPPY': 'å¼€å¿ƒ ğŸ˜Š',
        'SAD': 'æ‚²ä¼¤ ğŸ˜¢',
        'ANGRY': 'ç”Ÿæ°” ğŸ˜ ',
        'NEUTRAL': 'ä¸­æ€§ ğŸ˜',
        'SURPRISE': 'æƒŠè®¶ ğŸ˜²',
        'FEARFUL': 'ææƒ§ ğŸ˜°',
        'DISGUSTED': 'åŒæ¶ ğŸ¤¢',
    }
    
    # è¯­è¨€æ˜ å°„
    lang_map = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ja': 'æ—¥è¯­', 'ko': 'éŸ©è¯­', 'yue': 'ç²¤è¯­'}
    
    # äº‹ä»¶æ˜ å°„
    event_map = {
        'Speech': 'è¯­éŸ³',
        'Laughter': 'ç¬‘å£° ğŸ˜„',
        'Applause': 'æŒå£° ğŸ‘',
        'Cry': 'å“­æ³£ ğŸ˜­',
        'Music': 'éŸ³ä¹ ğŸµ',
    }
    
    # æå–æ ‡ç­¾
    pattern = r'<\|([^|]+)\|>'
    tags = re.findall(pattern, raw_text)
    clean_text = re.sub(pattern, '', raw_text).strip()
    
    language = tags[0] if len(tags) > 0 else 'unknown'
    emotion_raw = tags[1] if len(tags) > 1 else 'NEUTRAL'
    event_raw = tags[2] if len(tags) > 2 else 'Speech'
    
    emotion = emotion_map.get(emotion_raw, emotion_raw)
    event = event_map.get(event_raw, event_raw)
    lang = lang_map.get(language, language)
    
    print(f"   è½¬å½•: {clean_text}")
    print(f"   è¯­è¨€: {lang}")
    print(f"   æƒ…ç»ª: {emotion}")
    print(f"   äº‹ä»¶: {event}")
    print(f"   åŸå§‹: {raw_text}")
    
    # æ–‡æœ¬æƒ…æ„Ÿ
    text_sentiment = None
    if clean_text:
        try:
            from snownlp import SnowNLP
            s = SnowNLP(clean_text)
            text_sentiment = {
                'score': s.sentiments,
                'label': "æ­£é¢" if s.sentiments > 0.6 else "è´Ÿé¢" if s.sentiments < 0.4 else "ä¸­æ€§"
            }
            print(f"\nğŸ“Š æ–‡æœ¬æƒ…æ„Ÿ (SnowNLP): {text_sentiment['score']:.1%} â†’ {text_sentiment['label']}")
        except:
            pass
    
    return {
        'mode': 'SenseVoice',
        'transcript': clean_text,
        'language': lang,
        'emotion': emotion,
        'emotion_raw': emotion_raw,
        'event': event,
        'raw_output': raw_text,
        'text_sentiment': text_sentiment
    }


# ============== å¯¹æ¯”æ¨¡å¼ ==============
def run_both(audio_tensor):
    """åŒæ—¶è¿è¡Œä¸¤ç§æ¨¡å¼è¿›è¡Œå¯¹æ¯”"""
    print("\n" + "â–“" * 50)
    print("ã€å¯¹æ¯”æ¨¡å¼: åŒæ—¶æµ‹è¯•ä¸¤ç§æ–¹æ¡ˆã€‘")
    print("â–“" * 50)
    
    results = {}
    
    # Paraformer æ¨¡å¼
    try:
        load_paraformer()
        results['paraformer'] = run_paraformer(audio_tensor)
    except Exception as e:
        print(f"\nâŒ Paraformer å¤±è´¥: {e}")
        results['paraformer'] = None
    
    # SenseVoice æ¨¡å¼
    try:
        load_sensevoice()
        results['sensevoice'] = run_sensevoice(audio_tensor)
    except Exception as e:
        print(f"\nâŒ SenseVoice å¤±è´¥: {e}")
        results['sensevoice'] = None
    
    # å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
    print("=" * 50)
    
    print("\n{:<20} {:<25} {:<25}".format("é¡¹ç›®", "Paraformer+SpeechBrain", "SenseVoice"))
    print("-" * 70)
    
    p = results.get('paraformer') or {}
    s = results.get('sensevoice') or {}
    
    print("{:<20} {:<25} {:<25}".format(
        "è½¬å½•æ–‡æœ¬",
        (p.get('transcript', '-') or '-')[:20],
        (s.get('transcript', '-') or '-')[:20]
    ))
    print("{:<20} {:<25} {:<25}".format(
        "æƒ…ç»ªåˆ¤æ–­",
        p.get('emotion', '-'),
        s.get('emotion', '-')
    ))
    print("{:<20} {:<25} {:<25}".format(
        "ç½®ä¿¡åº¦/è¯¦æƒ…",
        f"{p.get('confidence', 0):.1%}" if p.get('confidence') else '-',
        s.get('event', '-')
    ))
    
    return results


# ============== ä¸»ç¨‹åº ==============
def main():
    parser = argparse.ArgumentParser(
        description='è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« - æ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python emotion_recognition_cli.py                 # äº¤äº’å¼é€‰æ‹©
  python emotion_recognition_cli.py -m paraformer  # Paraformer + SpeechBrain
  python emotion_recognition_cli.py -m sensevoice  # SenseVoice ä¸€ç«™å¼
  python emotion_recognition_cli.py -m both        # ä¸¤ç§éƒ½æµ‹è¯•å¯¹æ¯”
  python emotion_recognition_cli.py -d 10          # å½•éŸ³ 10 ç§’
        """
    )
    parser.add_argument('-m', '--mode', choices=['paraformer', 'sensevoice', 'both'],
                        help='é€‰æ‹©æ¨¡å‹æ¨¡å¼')
    parser.add_argument('-d', '--duration', type=int, default=RECORD_SECONDS,
                        help=f'å½•éŸ³æ—¶é•¿(ç§’), é»˜è®¤ {RECORD_SECONDS}')
    parser.add_argument('-f', '--file', type=str,
                        help='ç›´æ¥åˆ†æéŸ³é¢‘æ–‡ä»¶(è·³è¿‡å½•éŸ³)')
    
    args = parser.parse_args()
    
    # äº¤äº’å¼é€‰æ‹©æ¨¡å¼
    if args.mode is None:
        print("=" * 50)
        print("è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« v5.0")
        print("=" * 50)
        print("\né€‰æ‹©æ¨¡å‹æ¨¡å¼:")
        print("  1. Paraformer + SpeechBrain (ä¸­æ–‡è¯†åˆ«å‡†ï¼Œæƒ…æ„Ÿåˆ†å¼€)")
        print("  2. SenseVoice ä¸€ç«™å¼ (è¯­éŸ³+æƒ…æ„Ÿ+äº‹ä»¶æ£€æµ‹)")
        print("  3. ä¸¤ç§éƒ½æµ‹è¯•å¯¹æ¯”")
        print()
        
        while True:
            choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
            if choice == '1':
                args.mode = 'paraformer'
                break
            elif choice == '2':
                args.mode = 'sensevoice'
                break
            elif choice == '3':
                args.mode = 'both'
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1, 2 æˆ– 3")
    
    # åŠ è½½ VAD
    load_silero_vad()
    
    # åŠ è½½å¯¹åº”æ¨¡å‹
    if args.mode == 'paraformer':
        load_paraformer()
    elif args.mode == 'sensevoice':
        load_sensevoice()
    # both æ¨¡å¼åœ¨è¿è¡Œæ—¶åŠ è½½
    
    print("\n" + "=" * 50)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! æ¨¡å¼: {args.mode}")
    print("=" * 50)
    
    # ä¸»å¾ªç¯
    while True:
        try:
            input("\næŒ‰ Enter å¼€å§‹å½•éŸ³ (Ctrl+C é€€å‡º)...")
            
            # è·å–éŸ³é¢‘
            if args.file:
                print(f"\nğŸ“‚ åŠ è½½æ–‡ä»¶: {args.file}")
                import soundfile as sf
                audio_np, sr = sf.read(args.file)
                if sr != SAMPLE_RATE:
                    import librosa
                    audio_np = librosa.resample(audio_np, orig_sr=sr, target_sr=SAMPLE_RATE)
                audio = torch.from_numpy(audio_np.astype(np.float32))
            else:
                audio = capture_audio(args.duration)
            
            # VAD å¤„ç†
            print("\nğŸ” VAD è¯­éŸ³æ£€æµ‹...")
            speech_audio, timestamps = apply_vad(audio)
            
            if not timestamps:
                print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œè¯·é‡è¯•")
                continue
            
            # ä¿å­˜éŸ³é¢‘
            save_audio(speech_audio)
            
            # è¿è¡Œåˆ†æ
            if args.mode == 'paraformer':
                result = run_paraformer(speech_audio)
            elif args.mode == 'sensevoice':
                result = run_sensevoice(speech_audio)
            else:
                result = run_both(speech_audio)
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            print("\n" + "-" * 50)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºé€€å‡º")
            break


if __name__ == "__main__":
    main()
